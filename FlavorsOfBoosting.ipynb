{"cells":[{"cell_type":"markdown","metadata":{"id":"XnYzNLmInD3i"},"source":["### Flavors of Boosting\n","\n","In this notebook, we build a photometric redshift estimator using various boosting methods: AdaBoost and various flavors of Gradient Boosted Trees (GBM, HistGBM, and XGBoost). We also look at using RandomizedSearchCV in order to improve our exploration of parameter space.\n","\n","Our goal is to estimate photometric redshifts starting from observations of galaxy magnitudes in six different photometric bands (u, g, r, i, z, y). \n","\n","It accompanies Chapter 6 of the book (3 of 4).\n","\n","Copyright: Viviana Acquaviva (2023); see also other data credits below.\n","\n","License: [BSD-3-clause](https://opensource.org/license/bsd-3-clause/)\n","\n","Essentially, we try to reproduce/improve upon the results of [this paper](https://arxiv.org/abs/1903.08174), for which the data are public and available [here](http://d-scholarship.pitt.edu/36064/)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8JP6BVcnD3k"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from scipy import stats\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","pd.set_option('display.max_columns', 100)\n","pd.set_option('display.max_rows', 100)\n","pd.set_option('display.max_colwidth', 100)\n","\n","font = {'size'   : 16}\n","matplotlib.rc('font', **font)\n","matplotlib.rc('xtick', labelsize=14) \n","matplotlib.rc('ytick', labelsize=14) \n","#matplotlib.rcParams.update({'figure.autolayout': True})\n","matplotlib.rcParams['figure.dpi'] = 300"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fRr6XO89nD3k"},"outputs":[],"source":["from sklearn import metrics\n","from sklearn.model_selection import cross_validate, KFold, cross_val_predict, GridSearchCV\n","from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n","from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, GradientBoostingRegressor"]},{"cell_type":"markdown","metadata":{"id":"YaQTtIbOnD3m"},"source":["### We can read the data set with the selections applied in the previous notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6PVFlryPnD3n"},"outputs":[],"source":["sel_features = pd.read_csv('sel_features.csv', sep = '\\t')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"19HX5ncdnD3n"},"outputs":[],"source":["sel_target = pd.read_csv('sel_target.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BMUKkZUMnD3o","outputId":"3238f273-6736-4e61-f81e-4fa28b9a9ae5"},"outputs":[{"data":{"text/plain":["(6307, 6)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["sel_features.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NTpPKluPnD3p","outputId":"c8dde403-1e32-4074-a4aa-27d1d1bd482f"},"outputs":[{"data":{"text/plain":["array([1.0034, 0.9023, 0.4242, ..., 0.3703, 0.371 , 0.7333])"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["sel_target.values.ravel() #changes shape to 1d row-like array"]},{"cell_type":"markdown","metadata":{"id":"pCihwUFanD3p"},"source":["#### In the notebook \"BoostingDecisions\", we showed that for AdaBoost, stacking learners that are too weak doesn't help."]},{"cell_type":"markdown","metadata":{"id":"evMo-_xYnD3p"},"source":["This allows us to run a more informed parameter optimization."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ygJqRo-VnD3p","outputId":"860e38c0-4427-476b-d48f-154a89c0f334"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  \"timeout or by a memory leak.\", UserWarning\n"]},{"name":"stdout","output_type":"stream","text":["Best params, best score: 0.7581 {'base_estimator__max_depth': None, 'learning_rate': 0.3, 'loss': 'square', 'n_estimators': 100}\n"]}],"source":["# %%time Wall time on my machine was 1 min 17 s\n","\n","# Note: the variables after the \"time\" magic command are not updated - \n","# e.g. the \"model\" object will not be the one defined later in the cell.\n","# This may depend on the Jupyter notebook version.\n","\n","parameters = {'base_estimator__max_depth':[6,10,None], 'loss':['linear','square'], 'n_estimators':[20,50,100], 'learning_rate': [0.3,0.5,1.0]}\n","nmodels = np.product([len(el) for el in parameters.values()])\n","model = GridSearchCV(AdaBoostRegressor(base_estimator=DecisionTreeRegressor()), parameters, \\\n","                     cv = KFold(n_splits=5, shuffle=True, random_state = 5), \\\n","                     verbose = 2, n_jobs = 4, return_train_score=True)\n","model.fit(sel_features,sel_target.values.ravel())\n","\n","print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n","      model.best_params_)"]},{"cell_type":"markdown","metadata":{"id":"cyvakR19nD3q"},"source":["We can take a look at the winning model scores; in this case, we also pay attention to the standard deviation of test scores, because we want to know what differences are statistically significant when we compare different models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TsI06MgInD3q","outputId":"32aa34c0-ef9a-470b-9cfe-841b3257f028"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>params</th>\n","      <th>mean_test_score</th>\n","      <th>std_test_score</th>\n","      <th>mean_train_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>41</th>\n","      <td>{'base_estimator__max_depth': None, 'learning_rate': 0.3, 'loss': 'square', 'n_estimators': 100}</td>\n","      <td>0.758134</td>\n","      <td>0.032466</td>\n","      <td>0.999978</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>{'base_estimator__max_depth': None, 'learning_rate': 0.5, 'loss': 'square', 'n_estimators': 100}</td>\n","      <td>0.750198</td>\n","      <td>0.033006</td>\n","      <td>0.999732</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>{'base_estimator__max_depth': None, 'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 50}</td>\n","      <td>0.746901</td>\n","      <td>0.055565</td>\n","      <td>0.999746</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>{'base_estimator__max_depth': None, 'learning_rate': 0.3, 'loss': 'linear', 'n_estimators': 50}</td>\n","      <td>0.745610</td>\n","      <td>0.035138</td>\n","      <td>0.999992</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>{'base_estimator__max_depth': None, 'learning_rate': 1.0, 'loss': 'square', 'n_estimators': 50}</td>\n","      <td>0.743821</td>\n","      <td>0.038081</td>\n","      <td>0.986022</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                              params  \\\n","41  {'base_estimator__max_depth': None, 'learning_rate': 0.3, 'loss': 'square', 'n_estimators': 100}   \n","47  {'base_estimator__max_depth': None, 'learning_rate': 0.5, 'loss': 'square', 'n_estimators': 100}   \n","49   {'base_estimator__max_depth': None, 'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 50}   \n","37   {'base_estimator__max_depth': None, 'learning_rate': 0.3, 'loss': 'linear', 'n_estimators': 50}   \n","52   {'base_estimator__max_depth': None, 'learning_rate': 1.0, 'loss': 'square', 'n_estimators': 50}   \n","\n","    mean_test_score  std_test_score  mean_train_score  \n","41         0.758134        0.032466          0.999978  \n","47         0.750198        0.033006          0.999732  \n","49         0.746901        0.055565          0.999746  \n","37         0.745610        0.035138          0.999992  \n","52         0.743821        0.038081          0.986022  "]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["scores = pd.DataFrame(model.cv_results_)\n","scoresCV = scores[['params','mean_test_score','std_test_score','mean_train_score']].sort_values(by = 'mean_test_score', \\\n","                                                    ascending = False)\n","scoresCV.head()"]},{"cell_type":"markdown","metadata":{"id":"Ms4eDbtXnD3q"},"source":["We can see that the standard deviation is 0.03 - giving us a hint of what's significant - and that a few different models have similar scores. If you change the random seed in the cross validation, the scores will change by a similar amount, and the best model may change as well.\n","\n","Additionally, the resulting scores will not be exactly reproducible because there is another random component in the adaptive learning set (this means that if you run the cross_validate function using the best model from above, you might get a different average score!)"]},{"cell_type":"markdown","metadata":{"id":"yDy9kV4rnD3q"},"source":["#### Let's pick the best model and check the scores. We should do nested cross validation to get the generalization errors right - but if we are just comparing models, this is ok."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P-9rvW1CnD3r"},"outputs":[],"source":["bm = model.best_estimator_"]},{"cell_type":"markdown","metadata":{"id":"OpZBE-HWnD3r"},"source":["Let's generate predicted values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pfvwcY-3nD3s"},"outputs":[],"source":["y_pred_bm = cross_val_predict(bm, sel_features, sel_target.values.ravel(), cv = KFold(n_splits=5, shuffle = True, random_state=10))"]},{"cell_type":"markdown","metadata":{"id":"k5_B338xnD3s"},"source":["Calculate outlier fraction and NMAD:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TuLQb2J6nD3s","outputId":"f2b6fa9c-eb63-42e9-ce18-625791da3942"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.045\n","0.032\n"]}],"source":["print(np.round(len(np.where(np.abs(sel_target.values.ravel()-y_pred_bm)>0.15*(1+sel_target.values.ravel()))[0])/len(sel_target.values.ravel()),3))\n","\n","print(np.round(1.48*np.median(np.abs(sel_target.values.ravel()-y_pred_bm)/(1 + sel_target.values.ravel())),3))"]},{"cell_type":"markdown","metadata":{"id":"df3vHbnwnD3s"},"source":["These are actually better than what we obtained for the Random Forests model! But is the difference statistically significant? One way to explore this is by generating several sets of predictions, and calculating their standard deviation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"09XjPnfPnD3s","outputId":"437a5a99-775c-4b6c-d334-7f8326fc802d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 0\n","Iteration 1\n","Iteration 2\n","Iteration 3\n","Iteration 4\n","Iteration 5\n","Iteration 6\n","Iteration 7\n","OLF avg/std:, 0.04533, 0.00104\n","NMAD avg/std:, 0.03223, 0.00023\n"]}],"source":["seeds = np.random.choice(100,8, replace = False) #pick 8 different seeds\n","\n","olf = np.zeros(8)\n","NMAD = np.zeros(8)\n","\n","for i in range(8): #A bit rough, but it gives a sense of what happens by varying the random seeds!\n","    print('Iteration', i) #this is just to see the progress.\n","    ypred = cross_val_predict(bm, sel_features, sel_target.values.ravel(), cv = KFold(n_splits=5, shuffle=True, random_state=seeds[i]))\n","    olf[i] = len(np.where(np.abs(sel_target.values.ravel()-ypred)>0.15*(1+sel_target.values.ravel()))[0])/len(sel_target.values.ravel())\n","    NMAD[i] = 1.48*np.median(np.abs(sel_target.values.ravel()-ypred)/(1 + sel_target.values.ravel()))\n","\n","print('OLF avg/std:, {0:.5f}, {1:0.5f}'.format(olf.mean(), olf.std()))\n","print('NMAD avg/std:, {0:.5f}, {1:0.5f}'.format(NMAD.mean(), NMAD.std()))"]},{"cell_type":"markdown","metadata":{"id":"9mh9sFbFnD3u"},"source":["The result seems to be relatively solid, indicating that Boosting methods might be slightly better than RF when we take into account not just the R2 score, but the specific metrics we are monitoring for this problem."]},{"cell_type":"markdown","metadata":{"id":"bHFnPYvxnD3u"},"source":["### The next step is to compare Adaptive Boosting with different Gradient Boosted Trees algorithms. \n","\n","We begin by using sklearn's GBM, then we move on to the lighter version, HistGBM, and finally we consider one of the most popular GBT-based algorithm, XGBoost.\n","\n","We also take a look at the possibility of using a Randomized Search instead of a Grid Search in order to speed up our optimization process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GkoTHbBXnD3u"},"outputs":[],"source":["from sklearn.ensemble import GradientBoostingRegressor"]},{"cell_type":"markdown","metadata":{"id":"KfapuYR7nD3u"},"source":["The parameters depend on the particular implementation.\n","\n","In the sklearn formulation, the parameters of each tree are essentially the same we have for Random Forests; additionally we have the \"learning_rate\" parameter, which dictates how much each tree contribute to the final estimator, and the \"subsample\" parameters, which allows one to use a < 1.0 fraction of samples and introduce some regularization.\n"]},{"cell_type":"markdown","metadata":{"id":"EUhhsD7cnD3u"},"source":["### We can run the optimization process for this algorithm on a similar grid to the one used for AdaBoost."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r7efbOR_nD3u","outputId":"67f01f55-c635-4189-b60f-579a9c92d9c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  \"timeout or by a memory leak.\", UserWarning\n"]},{"name":"stdout","output_type":"stream","text":["Best params, best score: 0.7370 {'learning_rate': 0.1, 'loss': 'absolute_error', 'max_depth': None, 'n_estimators': 100}\n"]}],"source":["# %%time\n","# This took about 4.5 minutes on my machine\n","\n","parameters = {'max_depth':[6,10,None], 'loss':['squared_error','absolute_error'], \n","              'n_estimators':[20,50,100], 'learning_rate': [0.1,0.3,0.5]}\n","nmodels = np.product([len(el) for el in parameters.values()])\n","model = GridSearchCV(GradientBoostingRegressor(), parameters, \n","                     cv = KFold(n_splits=5, shuffle=True, random_state = 5), \\\n","                     verbose = 2, n_jobs = 4, return_train_score=True)\n","model.fit(sel_features,sel_target.values.ravel())\n","\n","print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n","      model.best_params_)"]},{"cell_type":"markdown","metadata":{"id":"t_HrnTwpnD3u"},"source":["These are comparable to what we obtained with AdaBoost (slightly lower, typically). We can check what happens to the outlier fraction and NMAD."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-6aDASOUnD3u"},"outputs":[],"source":["bm = model.best_estimator_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gu51uKFDnD3u"},"outputs":[],"source":["y_pred_bm = cross_val_predict(bm, sel_features, sel_target.values.ravel(), cv = KFold(n_splits=5, shuffle = True, random_state=10))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kWPhtufDnD3v","outputId":"16956666-4ae8-4295-8808-98f7d1099861"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.059\n","0.037\n"]}],"source":["print(np.round(len(np.where(np.abs(sel_target.values.ravel()-y_pred_bm)>0.15*(1+sel_target.values.ravel()))[0])/len(sel_target.values.ravel()),3))\n","\n","print(np.round(1.48*np.median(np.abs(sel_target.values.ravel()-y_pred_bm)/(1 + sel_target.values.ravel())),3))"]},{"cell_type":"markdown","metadata":{"id":"H93df3lSnD3v"},"source":["Overall, the performance of the two algorithms is similar, but one important difference is *timing*. To explore exactly the same parameter space, GBR took ~ 3 times longer than AdaBoost. Additionally, gradient boosted methods typically require more estimators, and we should explore more regularization parameters (e.g. subsampling) as well. In a nutshell, it would be great to speed up things."]},{"cell_type":"markdown","metadata":{"id":"ESF8IVEenD3v"},"source":["### How can we make things faster?"]},{"cell_type":"markdown","metadata":{"id":"W2RRi23EnD3v"},"source":["We can improve on the time constraints in two ways: by switching to the histogram-based version of Gradient Boosting Regressor, and by using a Random Search instead of a Grid Search.\n","\n","HistGradientBoostingRegressor (inspired by [LightGBM](https://lightgbm.readthedocs.io/en/latest/)) works by binning the features into integer-valued bins (the default value is 256, but this parameter can be adjusted; note however that 256 is the maximum!), which greatly reduces the number of splitting points to consider, and results in a vast reduction of computation time, especially for large data sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Styi7ZycnD3v"},"outputs":[],"source":["from sklearn.ensemble import HistGradientBoostingRegressor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0LwMtWr3nD3w","outputId":"0e2e6f6f-1cf9-41d1-a22b-d0457c9110cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 54 candidates, totalling 270 fits\n","Best params, best score: 0.7214 {'learning_rate': 0.3, 'loss': 'squared_error', 'max_depth': None, 'max_iter': 100}\n"]}],"source":["# %%time\n","# This took ~ 18s\n","\n","parameters = {'max_depth':[6,10,None], 'loss':['squared_error','absolute_error'], \n","              'max_iter':[20,50,100], 'learning_rate': [0.1,0.3,0.5]}\n","nmodels = np.product([len(el) for el in parameters.values()])\n","model = GridSearchCV(HistGradientBoostingRegressor(), parameters, \n","                     cv = KFold(n_splits=5, shuffle=True, random_state = 5), \\\n","                     verbose = 2, n_jobs = 4, return_train_score=True)\n","model.fit(sel_features,sel_target.values.ravel())\n","\n","print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n","      model.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16fHVf9_nD3w","outputId":"00d3ace3-bffe-4c09-c064-1deab9565527"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>params</th>\n","      <th>mean_test_score</th>\n","      <th>std_test_score</th>\n","      <th>mean_train_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>26</th>\n","      <td>{'learning_rate': 0.3, 'loss': 'squared_error', 'max_depth': None, 'max_iter': 100}</td>\n","      <td>0.721386</td>\n","      <td>0.036734</td>\n","      <td>0.934003</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>{'learning_rate': 0.3, 'loss': 'squared_error', 'max_depth': 6, 'max_iter': 100}</td>\n","      <td>0.719629</td>\n","      <td>0.031527</td>\n","      <td>0.889775</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>{'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': None, 'max_iter': 100}</td>\n","      <td>0.718633</td>\n","      <td>0.039798</td>\n","      <td>0.858452</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>{'learning_rate': 0.3, 'loss': 'squared_error', 'max_depth': 10, 'max_iter': 100}</td>\n","      <td>0.717640</td>\n","      <td>0.035547</td>\n","      <td>0.923584</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>{'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 10, 'max_iter': 100}</td>\n","      <td>0.715118</td>\n","      <td>0.039476</td>\n","      <td>0.843148</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                 params  \\\n","26  {'learning_rate': 0.3, 'loss': 'squared_error', 'max_depth': None, 'max_iter': 100}   \n","20     {'learning_rate': 0.3, 'loss': 'squared_error', 'max_depth': 6, 'max_iter': 100}   \n","8   {'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': None, 'max_iter': 100}   \n","23    {'learning_rate': 0.3, 'loss': 'squared_error', 'max_depth': 10, 'max_iter': 100}   \n","5     {'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 10, 'max_iter': 100}   \n","\n","    mean_test_score  std_test_score  mean_train_score  \n","26         0.721386        0.036734          0.934003  \n","20         0.719629        0.031527          0.889775  \n","8          0.718633        0.039798          0.858452  \n","23         0.717640        0.035547          0.923584  \n","5          0.715118        0.039476          0.843148  "]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["scores = pd.DataFrame(model.cv_results_)\n","scoresCV = scores[['params','mean_test_score','std_test_score','mean_train_score']].sort_values(by = 'mean_test_score', \\\n","                                                    ascending = False)\n","scoresCV.head()"]},{"cell_type":"markdown","metadata":{"id":"DBJR5eWhnD3w"},"source":["Even for this relatively small data set, this is much faster (about 15x faster than GradientBoostingRegressor), giving us a chance to explore a wider parameter space (e.g. more trees, more options for learning rate). The trade-off is that we obtain a slight decrease in performance, compared with GBR. However, the standard deviation of test scores over the 5 CV folds suggests that this difference is not statistically significant."]},{"cell_type":"markdown","metadata":{"id":"mdtL8YFZnD3w"},"source":["Let's explore a wider parameter space here:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjVPA9zwnD3w","outputId":"3c033cfe-93ad-4d9d-942f-b5043f15c71d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 144 candidates, totalling 720 fits\n","Best params, best score: 0.7356 {'early_stopping': False, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 10, 'max_iter': 500}\n"]}],"source":["#%%time\n","# This took 2 min 42 secs\n","\n","parameters = {'max_depth':[6,10,None], 'loss':['squared_error','absolute_error'], \n","              'max_iter':[100,200,500], 'learning_rate': [0.05, 0.1,0.3,0.5], \n","              'early_stopping':[True, False]}\n","nmodels = np.product([len(el) for el in parameters.values()])\n","model = GridSearchCV(HistGradientBoostingRegressor(), parameters, cv = KFold(n_splits=5, shuffle=True), \\\n","                     verbose = 2, n_jobs = 4, return_train_score=True)\n","model.fit(sel_features,sel_target.values.ravel())\n","\n","print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n","      model.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6txk7rWnD3y","outputId":"b4eb8ab7-c6d3-4969-ba7b-4264be834cf1"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>params</th>\n","      <th>mean_test_score</th>\n","      <th>std_test_score</th>\n","      <th>mean_train_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>95</th>\n","      <td>{'early_stopping': False, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 10, 'max_i...</td>\n","      <td>0.735611</td>\n","      <td>0.024161</td>\n","      <td>0.950685</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>{'early_stopping': False, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': None, 'max...</td>\n","      <td>0.734420</td>\n","      <td>0.025424</td>\n","      <td>0.960510</td>\n","    </tr>\n","    <tr>\n","      <th>80</th>\n","      <td>{'early_stopping': False, 'learning_rate': 0.05, 'loss': 'squared_error', 'max_depth': None, 'ma...</td>\n","      <td>0.731776</td>\n","      <td>0.024764</td>\n","      <td>0.923692</td>\n","    </tr>\n","    <tr>\n","      <th>115</th>\n","      <td>{'early_stopping': False, 'learning_rate': 0.3, 'loss': 'squared_error', 'max_depth': None, 'max...</td>\n","      <td>0.727963</td>\n","      <td>0.029271</td>\n","      <td>0.968498</td>\n","    </tr>\n","    <tr>\n","      <th>77</th>\n","      <td>{'early_stopping': False, 'learning_rate': 0.05, 'loss': 'squared_error', 'max_depth': 10, 'max_...</td>\n","      <td>0.726318</td>\n","      <td>0.023250</td>\n","      <td>0.909264</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                                  params  \\\n","95   {'early_stopping': False, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 10, 'max_i...   \n","98   {'early_stopping': False, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': None, 'max...   \n","80   {'early_stopping': False, 'learning_rate': 0.05, 'loss': 'squared_error', 'max_depth': None, 'ma...   \n","115  {'early_stopping': False, 'learning_rate': 0.3, 'loss': 'squared_error', 'max_depth': None, 'max...   \n","77   {'early_stopping': False, 'learning_rate': 0.05, 'loss': 'squared_error', 'max_depth': 10, 'max_...   \n","\n","     mean_test_score  std_test_score  mean_train_score  \n","95          0.735611        0.024161          0.950685  \n","98          0.734420        0.025424          0.960510  \n","80          0.731776        0.024764          0.923692  \n","115         0.727963        0.029271          0.968498  \n","77          0.726318        0.023250          0.909264  "]},"execution_count":71,"metadata":{},"output_type":"execute_result"}],"source":["scores = pd.DataFrame(model.cv_results_)\n","scoresCV = scores[['params','mean_test_score','std_test_score','mean_train_score']].sort_values(by = 'mean_test_score', \\\n","                                                    ascending = False)\n","scoresCV.head()"]},{"cell_type":"markdown","metadata":{"id":"VX9jfilinD3y"},"source":["HistGBR seems to improve (as expected) by using more trees and a smaller learning rate."]},{"cell_type":"markdown","metadata":{"id":"NAipuzcVnD3y"},"source":["### Comparison with Random Search"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8LG-l1ZnD3y"},"outputs":[],"source":["from sklearn.model_selection import RandomizedSearchCV"]},{"cell_type":"markdown","metadata":{"id":"oceBY2WfnD3y"},"source":["Finally, we can compare the performance and timings of the grid search above with the option of using a Randomized Search instead. We note that Random Search is usually preferable when we have a high-dimensional parameter space; its use is not particularly warranted here.\n","\n","The number of iterations (the number of models that are considered) also needs to be adjusted, and depends on the dimensionality of the parameter space as well as the functional dependence of the loss function on the parameters. We will compare the timings with the cell above, where we explore 144 models, and only use 30 for the random search."]},{"cell_type":"markdown","metadata":{"id":"VBmCjXGTnD3y"},"source":["The references here explores various ways of running a parameter search.\n","\n","Bergstra, J. and Bengio, Y., Random search for hyper-parameter optimization, The Journal of Machine Learning Research (2012)\n","\n","Bergstra, James, et al. \"Algorithms for hyper-parameter optimization.\" Advances in neural information processing systems 24 (2011)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NpFTKv-JnD3y","outputId":"34e35ee3-045f-439f-f708-e4a3c5644640"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 30 candidates, totalling 150 fits\n","Best params, best score: 0.7128 {'max_iter': 500, 'max_depth': None, 'loss': 'absolute_error', 'learning_rate': 0.1, 'early_stopping': True}\n"]}],"source":["#%%time\n","# 31 seconds\n","\n","parameters = {'max_depth':[6,10,None], 'loss':['squared_error','absolute_error'], \n","              'max_iter':[100,200,500], 'learning_rate': [0.05, 0.1,0.3,0.5], \n","             'early_stopping':[True, False]}\n","nmodels = np.product([len(el) for el in parameters.values()])\n","model = RandomizedSearchCV(HistGradientBoostingRegressor(), parameters, cv = KFold(n_splits=5, shuffle=True), \\\n","                     verbose = 2, n_jobs = 4, return_train_score=True, n_iter=30)\n","model.fit(sel_features,sel_target.values.ravel())\n","\n","print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n","      model.best_params_)"]},{"cell_type":"markdown","metadata":{"id":"dQ1HoXQanD3y"},"source":["The Randomized Search was able to find a comparably good solution in less than 1/5 of the time. As we mentioned, the true gains of a Randomized Search pertain to exploring high-dimensional spaces. It is also possible to use the Randomized Search to find the general area of optimal parameters, and then refine the search in that neighborhood with a finer Grid Search."]},{"cell_type":"markdown","metadata":{"id":"1M_3MB0CnD3y"},"source":["### Finally, we can compare with XGBoost."]},{"cell_type":"markdown","metadata":{"id":"IYtnXEconD3y"},"source":["[XGBoost](https://xgboost.readthedocs.io/en/latest/index.html#) stands for “Extreme Gradient Boosting”. It is sometimes known as \"regularized\" GBM, as it has a default regularization term on the weights of the ensemble, and is more robust to overfitting. It has more flexibility in defining weak learners, as well as the objective (loss) function (note that this doesn't apply to the base estimators, e.g. how splits in trees are chosen, but on the loss that is used to compute pseudoresiduals and gradients). \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WqgQCb0EnD3z"},"outputs":[],"source":["import xgboost as xgb"]},{"cell_type":"markdown","metadata":{"id":"TI5v8EOGnD3z"},"source":["Medium article explaining XGBoost: [here](https://towardsdatascience.com/a-beginners-guide-to-xgboost-87f5d4c30ed7); some nice tutorials from XGBoost's site: [here](https://xgboost.readthedocs.io/en/latest/tutorials/index.html)"]},{"cell_type":"markdown","metadata":{"id":"P52BMto0nD3z"},"source":["We can begin by using Grid Search and the original parameter space, in order to compare timings with GBM and HistGBM."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kWe7diGMnD3z"},"outputs":[],"source":["# %%time\n","\n","# This took ~ 3 minutes\n","\n","parameters = {'max_depth':[6,10,None], 'n_estimators':[50,100,200], \n","              'learning_rate': [0.1, 0.3,0.5], 'objective':['reg:squarederror','reg:squaredlogerror']}\n","\n","\n","nmodels = np.product([len(el) for el in parameters.values()])\n","model = GridSearchCV(xgb.XGBRegressor(), parameters, cv = KFold(n_splits=5, shuffle=True), \\\n","                     verbose = 2, n_jobs = 4, return_train_score=True)\n","model.fit(sel_features,sel_target.values.ravel())\n","\n","print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n","      model.best_params_)"]},{"cell_type":"markdown","metadata":{"id":"goZWl1ZSnD3z"},"source":["XGBoost is slightly more efficient than GBM, and achieves comparable results on a similar grid. We can use the Random Search to explore some more intensive models (more trees, lower learning rate), and add subsampling as an extra form of regularization."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4PZ8LAOKnD3z","outputId":"b507b40d-b6ad-4869-fe9c-9c2870c5028a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  \"timeout or by a memory leak.\", UserWarning\n"]},{"name":"stdout","output_type":"stream","text":["Best params, best score: 0.7424 {'subsample': 0.5, 'objective': 'reg:squarederror', 'n_estimators': 500, 'max_depth': 10, 'learning_rate': 0.02}\n","CPU times: user 18.1 s, sys: 2.8 s, total: 20.9 s\n","Wall time: 2min 15s\n"]}],"source":["%%time\n","\n","# 3 min 36 secs\n","\n","parameters = {'max_depth':[6,10,None], 'n_estimators':[50, 100, 200,500], \n","              'learning_rate': [0.02,0.05,0.1,0.3], 'objective':['reg:squarederror',\n","                'reg:squaredlogerror'], 'subsample':[0.5,1]}\n","\n","nmodels = np.product([len(el) for el in parameters.values()])\n","model = RandomizedSearchCV(xgb.XGBRegressor(), parameters, cv = KFold(n_splits=5, shuffle=True), \\\n","                     verbose = 2, n_jobs = 4, return_train_score=True, n_iter = 30)\n","model.fit(sel_features,sel_target.values.ravel())\n","\n","print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n","      model.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-jQCtAXEnD30","outputId":"73a81883-740c-4a0e-c218-1551931d2594"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>params</th>\n","      <th>mean_test_score</th>\n","      <th>std_test_score</th>\n","      <th>mean_train_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>10</th>\n","      <td>{'subsample': 0.5, 'objective': 'reg:squaredlogerror', 'n_estimators': 500, 'max_depth': 10, 'le...</td>\n","      <td>0.750818</td>\n","      <td>0.073358</td>\n","      <td>0.952617</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>{'subsample': 0.5, 'objective': 'reg:squaredlogerror', 'n_estimators': 200, 'max_depth': 10, 'le...</td>\n","      <td>0.750432</td>\n","      <td>0.074676</td>\n","      <td>0.940629</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>{'subsample': 1, 'objective': 'reg:squaredlogerror', 'n_estimators': 50, 'max_depth': 10, 'learn...</td>\n","      <td>0.733495</td>\n","      <td>0.074933</td>\n","      <td>0.920086</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>{'subsample': 1, 'objective': 'reg:squaredlogerror', 'n_estimators': 100, 'max_depth': 10, 'lear...</td>\n","      <td>0.733070</td>\n","      <td>0.068503</td>\n","      <td>0.972264</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>{'subsample': 1, 'objective': 'reg:squaredlogerror', 'n_estimators': 50, 'max_depth': 10, 'learn...</td>\n","      <td>0.732854</td>\n","      <td>0.068384</td>\n","      <td>0.951915</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                                 params  \\\n","10  {'subsample': 0.5, 'objective': 'reg:squaredlogerror', 'n_estimators': 500, 'max_depth': 10, 'le...   \n","16  {'subsample': 0.5, 'objective': 'reg:squaredlogerror', 'n_estimators': 200, 'max_depth': 10, 'le...   \n","20  {'subsample': 1, 'objective': 'reg:squaredlogerror', 'n_estimators': 50, 'max_depth': 10, 'learn...   \n","6   {'subsample': 1, 'objective': 'reg:squaredlogerror', 'n_estimators': 100, 'max_depth': 10, 'lear...   \n","22  {'subsample': 1, 'objective': 'reg:squaredlogerror', 'n_estimators': 50, 'max_depth': 10, 'learn...   \n","\n","    mean_test_score  std_test_score  mean_train_score  \n","10         0.750818        0.073358          0.952617  \n","16         0.750432        0.074676          0.940629  \n","20         0.733495        0.074933          0.920086  \n","6          0.733070        0.068503          0.972264  \n","22         0.732854        0.068384          0.951915  "]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["scores = pd.DataFrame(model.cv_results_)\n","scoresCV = scores[['params','mean_test_score','std_test_score','mean_train_score']].sort_values(by = 'mean_test_score', \\\n","                                                    ascending = False)\n","scoresCV.head()"]},{"cell_type":"markdown","metadata":{"id":"X_QCDhlBnD30"},"source":["We are able to get slightly higher scores using this wider parameter space in combination with the Randomized Search, but again, the statistical significance of this increase is very low."]},{"cell_type":"markdown","metadata":{"id":"s1W_3b82nD30"},"source":["We can also look for outlier fraction and NMAD:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bTQ9nq2RnD30"},"outputs":[],"source":["y_pred_bm = cross_val_predict(bm, sel_features, sel_target.values.ravel(), cv = KFold(n_splits=5, shuffle = True, random_state=5))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IXUNuKYKnD30","outputId":"008fabcf-84d3-477c-cb77-64c5391d285a"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.056128111622007294\n","0.03729988572517498\n"]}],"source":["print(len(np.where(np.abs(sel_target.values.ravel()-y_pred_bm)>0.15*(1+sel_target.values.ravel()))[0])/len(sel_target.values.ravel()))\n","\n","print(1.48*np.median(np.abs(sel_target.values.ravel()-y_pred_bm)/(1 + sel_target.values.ravel())))"]},{"cell_type":"markdown","metadata":{"id":"wBymPKgtnD30"},"source":["### Conclusion: all boosting algorithms behave fairly similarly for this data set. It might be worth simply using the fastest one (HistGBR + Random Search)."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}